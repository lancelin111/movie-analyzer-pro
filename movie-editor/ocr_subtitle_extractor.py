#!/usr/bin/env python3
"""
OCRå­—å¹•æå–å™¨
ä¸“é—¨ç”¨äºè¯†åˆ«è§†é¢‘ä¸­çš„ç¡¬å­—å¹•ï¼ˆçƒ§å½•å­—å¹•ï¼‰
æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šè¯­è¨€
"""

import cv2
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import subprocess
import json

# å°è¯•å¯¼å…¥ä¸åŒçš„OCRåº“
OCR_AVAILABLE = None
try:
    import easyocr
    OCR_AVAILABLE = "easyocr"
    print("âœ… ä½¿ç”¨ EasyOCRï¼ˆæ¨èï¼Œè¯†åˆ«ç‡é«˜ï¼‰")
except ImportError:
    try:
        import pytesseract
        from PIL import Image
        OCR_AVAILABLE = "pytesseract"
        print("âš ï¸ ä½¿ç”¨ Pytesseractï¼ˆéœ€è¦å®‰è£…tesseractï¼‰")
    except ImportError:
        print("âŒ æœªå®‰è£…OCRåº“ï¼Œè¯·å®‰è£… easyocr æˆ– pytesseract")

class OCRSubtitleExtractor:
    """OCRå­—å¹•æå–å™¨"""
    
    def __init__(self, video_path: str, language: str = "zh"):
        """
        åˆå§‹åŒ–OCRæå–å™¨
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            language: è¯­è¨€ä»£ç  (zh=ä¸­æ–‡, en=è‹±æ–‡, ko=éŸ©è¯­)
        """
        self.video_path = Path(video_path)
        self.language = language
        self.cap = None
        self.fps = 0
        self.frame_count = 0
        self.width = 0
        self.height = 0
        
        # åˆå§‹åŒ–OCR
        self._init_ocr()
        
    def _init_ocr(self):
        """åˆå§‹åŒ–OCRå¼•æ“"""
        if OCR_AVAILABLE == "easyocr":
            # EasyOCRè¯­è¨€æ˜ å°„
            lang_map = {
                'zh': ['ch_sim', 'en'],  # ç®€ä½“ä¸­æ–‡+è‹±æ–‡
                'en': ['en'],
                'ko': ['ko', 'en'],
                'ja': ['ja', 'en']
            }
            languages = lang_map.get(self.language, ['ch_sim', 'en'])
            
            print(f"ğŸ”„ åˆå§‹åŒ– EasyOCR (è¯­è¨€: {languages})...")
            self.reader = easyocr.Reader(languages, gpu=False)
            
        elif OCR_AVAILABLE == "pytesseract":
            # Tesseractè¯­è¨€æ˜ å°„
            self.tesseract_lang = {
                'zh': 'chi_sim',
                'en': 'eng',
                'ko': 'kor',
                'ja': 'jpn'
            }.get(self.language, 'chi_sim')
            
            print(f"ğŸ”„ åˆå§‹åŒ– Pytesseract (è¯­è¨€: {self.tesseract_lang})...")
    
    def extract_subtitles(self, 
                         duration_limit: int = 600,
                         sample_interval: float = 1.0,
                         subtitle_region: Tuple[float, float] = (0.6, 1.0)) -> List[Dict]:
        """
        æå–ç¡¬å­—å¹•
        
        Args:
            duration_limit: åˆ†ææ—¶é•¿é™åˆ¶ï¼ˆç§’ï¼‰
            sample_interval: é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
            subtitle_region: å­—å¹•åŒºåŸŸ (é¡¶éƒ¨æ¯”ä¾‹, åº•éƒ¨æ¯”ä¾‹)ï¼Œé»˜è®¤åº•éƒ¨40%
        
        Returns:
            å­—å¹•åˆ—è¡¨
        """
        print(f"\nğŸ¯ å¼€å§‹OCRå­—å¹•æå–...")
        print(f"  â€¢ è§†é¢‘: {self.video_path.name}")
        print(f"  â€¢ è¯­è¨€: {self.language}")
        print(f"  â€¢ é‡‡æ ·é—´éš”: {sample_interval}ç§’")
        print(f"  â€¢ å­—å¹•åŒºåŸŸ: ç”»é¢åº•éƒ¨{(1-subtitle_region[0])*100:.0f}%")
        
        # æ‰“å¼€è§†é¢‘
        self.cap = cv2.VideoCapture(str(self.video_path))
        self.fps = self.cap.get(cv2.CAP_PROP_FPS)
        self.frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        print(f"  â€¢ è§†é¢‘ä¿¡æ¯: {self.width}x{self.height} @ {self.fps:.2f}fps")
        
        # è®¡ç®—é‡‡æ ·å¸§
        sample_frames = int(self.fps * sample_interval)
        max_frames = min(int(duration_limit * self.fps), self.frame_count)
        
        subtitles = []
        last_text = ""
        last_start = 0
        processed_frames = 0
        
        print(f"\nğŸ“Š å¤„ç†è¿›åº¦:")
        
        for frame_idx in range(0, max_frames, sample_frames):
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = self.cap.read()
            
            if not ret:
                break
            
            processed_frames += 1
            current_time = frame_idx / self.fps
            
            # æ¯10ç§’æ˜¾ç¤ºè¿›åº¦
            if processed_frames % 10 == 0:
                progress = (frame_idx / max_frames) * 100
                print(f"  å¤„ç†ä¸­... {progress:.1f}% ({current_time:.1f}ç§’)")
            
            # æå–å­—å¹•åŒºåŸŸ
            subtitle_frame = self._extract_subtitle_region(frame, subtitle_region)
            
            # é¢„å¤„ç†å›¾åƒ
            processed_frame = self._preprocess_frame(subtitle_frame)
            
            # OCRè¯†åˆ«
            text = self._ocr_frame(processed_frame)
            
            # æ¸…ç†æ–‡æœ¬
            text = self._clean_text(text)
            
            # å¦‚æœè¯†åˆ«åˆ°æ–°æ–‡æœ¬
            if text and text != last_text:
                # ä¿å­˜ä¸Šä¸€æ¡å­—å¹•
                if last_text:
                    subtitles.append({
                        'id': len(subtitles),
                        'start': last_start,
                        'end': current_time,
                        'text': last_text,
                        'confidence': 0.8  # ç½®ä¿¡åº¦ï¼ˆå¦‚æœOCRæä¾›çš„è¯ï¼‰
                    })
                
                last_text = text
                last_start = current_time
        
        # æ·»åŠ æœ€åä¸€æ¡å­—å¹•
        if last_text:
            subtitles.append({
                'id': len(subtitles),
                'start': last_start,
                'end': max_frames / self.fps,
                'text': last_text,
                'confidence': 0.8
            })
        
        self.cap.release()
        
        print(f"\nâœ… OCRæå–å®Œæˆ!")
        print(f"  â€¢ è¯†åˆ«å­—å¹•æ•°: {len(subtitles)}")
        
        # æ˜¾ç¤ºå‰3æ¡å­—å¹•
        if subtitles:
            print(f"\nğŸ“ å­—å¹•ç¤ºä¾‹:")
            for i, sub in enumerate(subtitles[:3], 1):
                print(f"  {i}. [{sub['start']:.1f}s] {sub['text'][:50]}...")
        
        return subtitles
    
    def _extract_subtitle_region(self, frame: np.ndarray, 
                                region: Tuple[float, float]) -> np.ndarray:
        """æå–å­—å¹•åŒºåŸŸ"""
        h, w = frame.shape[:2]
        
        # è®¡ç®—å­—å¹•åŒºåŸŸ
        top = int(h * region[0])
        bottom = int(h * region[1])
        
        # æå–åŒºåŸŸ
        subtitle_region = frame[top:bottom, :]
        
        return subtitle_region
    
    def _preprocess_frame(self, frame: np.ndarray) -> np.ndarray:
        """é¢„å¤„ç†å›¾åƒä»¥æé«˜OCRè¯†åˆ«ç‡"""
        # è½¬æ¢ä¸ºç°åº¦å›¾
        if len(frame.shape) == 3:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        else:
            gray = frame
        
        # æ–¹æ³•1ï¼šäºŒå€¼åŒ–ï¼ˆé€‚åˆç™½è‰²å­—å¹•ï¼‰
        _, binary = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)
        
        # æ–¹æ³•2ï¼šè‡ªé€‚åº”é˜ˆå€¼ï¼ˆé€‚åˆå¤æ‚èƒŒæ™¯ï¼‰
        # binary = cv2.adaptiveThreshold(gray, 255, 
        #                               cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        #                               cv2.THRESH_BINARY, 11, 2)
        
        # å»å™ª
        denoised = cv2.medianBlur(binary, 3)
        
        # æ”¾å¤§ä»¥æé«˜è¯†åˆ«ç‡
        scaled = cv2.resize(denoised, None, fx=2, fy=2, 
                           interpolation=cv2.INTER_CUBIC)
        
        return scaled
    
    def _ocr_frame(self, frame: np.ndarray) -> str:
        """å¯¹å•å¸§è¿›è¡ŒOCRè¯†åˆ«"""
        if OCR_AVAILABLE == "easyocr":
            # EasyOCRè¯†åˆ«
            results = self.reader.readtext(frame, detail=0)
            text = ' '.join(results)
            
        elif OCR_AVAILABLE == "pytesseract":
            # Pytesseractè¯†åˆ«
            text = pytesseract.image_to_string(
                frame,
                lang=self.tesseract_lang,
                config='--psm 7'  # å•è¡Œæ–‡æœ¬æ¨¡å¼
            )
        else:
            text = ""
        
        return text.strip()
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†OCRè¯†åˆ«çš„æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆå¯é€‰ï¼‰
        # text = re.sub(r'[^\w\s\u4e00-\u9fa5]', '', text)
        
        # ç§»é™¤å¤ªçŸ­çš„æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯è¯¯è¯†åˆ«ï¼‰
        if len(text) < 2:
            return ""
        
        return text
    
    def extract_with_multiple_methods(self, duration_limit: int = 600) -> Dict:
        """
        ä½¿ç”¨å¤šç§æ–¹æ³•æå–å­—å¹•ï¼Œè¿”å›æœ€ä½³ç»“æœ
        """
        results = {}
        
        # æ–¹æ³•1ï¼šåº•éƒ¨40%åŒºåŸŸ
        print("\nğŸ” æ–¹æ³•1: æ‰«æåº•éƒ¨40%åŒºåŸŸ...")
        subtitles_bottom = self.extract_subtitles(
            duration_limit=duration_limit,
            subtitle_region=(0.6, 1.0)
        )
        results['bottom'] = subtitles_bottom
        
        # æ–¹æ³•2ï¼šåº•éƒ¨20%åŒºåŸŸï¼ˆæ›´ç²¾ç¡®ï¼‰
        print("\nğŸ” æ–¹æ³•2: æ‰«æåº•éƒ¨20%åŒºåŸŸ...")
        subtitles_narrow = self.extract_subtitles(
            duration_limit=duration_limit,
            subtitle_region=(0.8, 1.0)
        )
        results['narrow'] = subtitles_narrow
        
        # é€‰æ‹©æœ€ä½³ç»“æœï¼ˆå­—å¹•æ•°é‡æœ€å¤šçš„ï¼‰
        best_method = max(results.keys(), key=lambda k: len(results[k]))
        best_subtitles = results[best_method]
        
        print(f"\nâœ¨ æœ€ä½³æ–¹æ³•: {best_method} (è¯†åˆ«{len(best_subtitles)}æ¡å­—å¹•)")
        
        return {
            'subtitles': best_subtitles,
            'method': best_method,
            'all_results': results
        }

def test_ocr_extraction(video_path: str, language: str = "zh"):
    """æµ‹è¯•OCRå­—å¹•æå–"""
    print("="*60)
    print("ğŸ¯ OCRå­—å¹•æå–æµ‹è¯•")
    print("="*60)
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    if not Path(video_path).exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return
    
    # åˆ›å»ºæå–å™¨
    extractor = OCRSubtitleExtractor(video_path, language)
    
    # æå–å­—å¹•ï¼ˆæµ‹è¯•30ç§’ï¼‰
    subtitles = extractor.extract_subtitles(
        duration_limit=30,
        sample_interval=1.0,
        subtitle_region=(0.7, 1.0)  # åº•éƒ¨30%
    )
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("output/ocr_test")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ä¸ºSRT
    srt_path = output_dir / "ocr_subtitles.srt"
    with open(srt_path, 'w', encoding='utf-8') as f:
        for i, sub in enumerate(subtitles, 1):
            f.write(f"{i}\n")
            f.write(f"{format_time(sub['start'])} --> {format_time(sub['end'])}\n")
            f.write(f"{sub['text']}\n\n")
    
    print(f"\nâœ… å­—å¹•å·²ä¿å­˜: {srt_path}")
    
    # ä¿å­˜JSON
    json_path = output_dir / "ocr_subtitles.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(subtitles, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… JSONå·²ä¿å­˜: {json_path}")

def format_time(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´ä¸ºSRTæ ¼å¼"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

def install_requirements():
    """å®‰è£…æ‰€éœ€ä¾èµ–"""
    print("ğŸ“¦ å®‰è£…OCRä¾èµ–...")
    
    # å®‰è£…easyocr
    subprocess.run(["pip3", "install", "easyocr"], check=False)
    
    # æˆ–å®‰è£…pytesseract
    # subprocess.run(["pip3", "install", "pytesseract", "pillow"], check=False)
    # macOS: brew install tesseract tesseract-lang
    # Ubuntu: sudo apt-get install tesseract-ocr tesseract-ocr-chi-sim

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        video_path = sys.argv[1]
        language = sys.argv[2] if len(sys.argv) > 2 else "zh"
        test_ocr_extraction(video_path, language)
    else:
        # æµ‹è¯•ç”Ÿä¸‡ç‰©è§†é¢‘
        test_video = "/Users/linguangjie/movie-editor/ç”Ÿä¸‡ç‰©-01.mp4"
        if Path(test_video).exists():
            test_ocr_extraction(test_video, "zh")
        else:
            print("ç”¨æ³•: python ocr_subtitle_extractor.py <è§†é¢‘è·¯å¾„> [è¯­è¨€]")
            print("ç¤ºä¾‹: python ocr_subtitle_extractor.py video.mp4 zh")